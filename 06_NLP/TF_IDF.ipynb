{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsRF/odpllk/2Hwatdi3fv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SakshamRimal/Deep-Learning/blob/main/06_NLP/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SuEoSOBhkkTl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIQxRfHwmavL",
        "outputId": "89a9db09-c9a0-447d-8b19-5d577108115f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXRDo0OamdAZ",
        "outputId": "32a2c944-796a-41c0-b52f-ccd45de510c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"I love programming in Python\",\n",
        "    \"Python programming is Fun\",\n",
        "    \"I love machine learning\"\n",
        "]"
      ],
      "metadata": {
        "id": "Ro8hIBmcmnq9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "yo0JMaaomwLr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_docs = [\n",
        "    \" \".join([word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words])\n",
        "    for doc in documents\n",
        "]"
      ],
      "metadata": {
        "id": "C3qt8aEim1Ku"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bccff10",
        "outputId": "2445018c-9377-416f-9113-bc68dfe610c1"
      },
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNZkzUJ3ndus",
        "outputId": "ff2d6e57-f7c7-4412-f42e-64419478f9d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love programming python', 'python programming fun', 'love machine learning']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "WalzFtMqneoF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix = vectorizer.fit_transform(filtered_docs)"
      ],
      "metadata": {
        "id": "1jcCFgwgnnKl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Matrix:\\n\", df_tfidf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjDVGmCtny_N",
        "outputId": "67328787-b359-46a9-80db-be7120f9b4d1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            "         fun  learning     love   machine  programming    python\n",
            "0  0.000000  0.000000  0.57735  0.000000     0.577350  0.577350\n",
            "1  0.680919  0.000000  0.00000  0.000000     0.517856  0.517856\n",
            "2  0.000000  0.622766  0.47363  0.622766     0.000000  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_docs = [\" \".join(words) for words in filtered_docs]\n",
        "\n",
        "print(\"Cleaned Documents:\", clean_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBDSB2YPoBf6",
        "outputId": "7392f07a-80d0-491d-e568-4615f96988c4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Documents: ['l o v e   p r o g r a m m i n g   p y t h o n', 'p y t h o n   p r o g r a m m i n g   f u n', 'l o v e   m a c h i n e   l e a r n i n g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#term frequency\n",
        "count_vectorizer = vectorizer.build_analyzer()\n",
        "tf_values = []\n",
        "for doc in clean_docs:\n",
        "    word_counts = {}\n",
        "    words = count_vectorizer(doc)\n",
        "    for word in words:\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1\n",
        "    doc_len = len(words)\n",
        "    tf_values.append({word: count / doc_len for word, count in word_counts.items()})\n",
        "print(\"\\nTerm Frequencies (TF):\")\n",
        "for i, tf in enumerate(tf_values):\n",
        "    print(f\"Doc {i+1}:\", tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWWWhs0poHgk",
        "outputId": "2db0f596-43a8-4dc6-e522-228be36fae6a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Term Frequencies (TF):\n",
            "Doc 1: {}\n",
            "Doc 2: {}\n",
            "Doc 3: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inverse document frequency\n",
        "idf_scores = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
        "print(\"\\nInverse Document Frequency (IDF):\")\n",
        "for word, score in idf_scores.items():\n",
        "    print(f\"{word}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R3qwI7WoKeE",
        "outputId": "faec7b81-2c78-4b14-9e25-ffb8d6797753"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inverse Document Frequency (IDF):\n",
            "fun: 1.6931\n",
            "learning: 1.6931\n",
            "love: 1.2877\n",
            "machine: 1.6931\n",
            "programming: 1.2877\n",
            "python: 1.2877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTF-IDF scores for each document:\")\n",
        "for i, row in enumerate(tfidf_matrix.toarray()):\n",
        "    print(f\"Doc {i+1}:\")\n",
        "    for word, score in zip(vectorizer.get_feature_names_out(), row):\n",
        "        if score > 0:\n",
        "            print(f\"  {word}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6Xu2aR2oVW5",
        "outputId": "f7c04cea-a446-4bff-fdb0-cbf2b808f8b0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF scores for each document:\n",
            "Doc 1:\n",
            "  love: 0.5774\n",
            "  programming: 0.5774\n",
            "  python: 0.5774\n",
            "Doc 2:\n",
            "  fun: 0.6809\n",
            "  programming: 0.5179\n",
            "  python: 0.5179\n",
            "Doc 3:\n",
            "  learning: 0.6228\n",
            "  love: 0.4736\n",
            "  machine: 0.6228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How this program uses all TF-IDF concepts\n",
        "\n",
        "    Tokenization → nltk.word_tokenize\n",
        "\n",
        "    Lowercasing & Punctuation Removal → .isalpha()\n",
        "\n",
        "    Stopword Removal → stopwords.words('english')\n",
        "\n",
        "    TF Calculation → Term counts ÷ total words in document\n",
        "\n",
        "    IDF Calculation → idf = log(N / (1 + df)) + 1\n",
        "\n",
        "    TF-IDF Matrix → Generated using TfidfVectorizer\n",
        "\n",
        "    Readable Output → pandas.DataFrame for matrix & dictionary for TF/IDF"
      ],
      "metadata": {
        "id": "AGB5xj-8oeHc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VLROsjI2oetN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}